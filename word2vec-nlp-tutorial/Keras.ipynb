{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"clean_train_reviewsbg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>cleanReview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "      <td>stuff going moment mj started listening music ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review  \\\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...   \n",
       "\n",
       "                                         cleanReview  \n",
       "0  stuff going moment mj started listening music ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(train['cleanReview'], train['sentiment'], test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_obj = Tokenizer()\n",
    "total_reviews = train['cleanReview']\n",
    "tokenizer_obj.fit_on_texts(total_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(s.split()) for s in total_reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer_obj.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtraintok = tokenizer_obj.texts_to_sequences(train_features)\n",
    "Xtesttok = tokenizer_obj.texts_to_sequences(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8250\n",
      "8250\n"
     ]
    }
   ],
   "source": [
    "X_train_pad = pad_sequences(Xtraintok, maxlen=max_length, padding='post')\n",
    "X_test_pad = pad_sequences(Xtesttok, maxlen=max_length, padding='post')\n",
    "print(len(X_test_pad))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Dense, LSTM, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "from keras import optimizers\n",
    "### word embedding mapping a word a learn repesnentaion: an array #numeric space is not meaningful us for people "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100  ##negative or positive a word is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length))\n",
    "model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "## entire review 10 words by 10 x 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ngmat\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1416, 100)         7406600   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 7,419,401\n",
      "Trainable params: 7,419,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16750 samples, validate on 8250 samples\n",
      "Epoch 1/3\n",
      " - 394s - loss: 0.6934 - val_loss: 0.6938\n",
      "Epoch 2/3\n",
      " - 412s - loss: 0.6935 - val_loss: 0.6934\n",
      "Epoch 3/3\n",
      " - 414s - loss: 0.6933 - val_loss: 0.6935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x23c2887d9b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_pad, train_labels, batch_size=100, epochs=3, validation_data=(X_test_pad, test_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_pred_train = model.predict(X_train_pad)\n",
    "sen_pred_test = model.predict(X_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4892042 ]\n",
      " [0.4892042 ]\n",
      " [0.4892042 ]\n",
      " ...\n",
      " [0.4892042 ]\n",
      " [0.48920423]\n",
      " [0.48920423]]\n"
     ]
    }
   ],
   "source": [
    "print(sen_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6868     0\n",
      "24016    1\n",
      "9668     0\n",
      "13640    1\n",
      "14018    0\n",
      "7488     1\n",
      "5804     1\n",
      "12909    0\n",
      "3386     0\n",
      "9567     0\n",
      "21423    0\n",
      "3503     0\n",
      "6657     0\n",
      "19192    1\n",
      "2518     1\n",
      "13361    1\n",
      "21135    0\n",
      "21080    1\n",
      "6492     0\n",
      "6891     0\n",
      "7752     1\n",
      "8614     0\n",
      "6649     1\n",
      "2227     1\n",
      "12060    1\n",
      "19122    0\n",
      "9622     1\n",
      "12279    1\n",
      "16447    0\n",
      "423      1\n",
      "        ..\n",
      "47       1\n",
      "8956     0\n",
      "13267    1\n",
      "24606    0\n",
      "12133    1\n",
      "13093    0\n",
      "17555    1\n",
      "12509    1\n",
      "3289     1\n",
      "23121    0\n",
      "4755     1\n",
      "16426    0\n",
      "18307    1\n",
      "16178    0\n",
      "18516    0\n",
      "23198    1\n",
      "11472    0\n",
      "15436    1\n",
      "9048     0\n",
      "1871     0\n",
      "23068    1\n",
      "22818    0\n",
      "418      0\n",
      "1213     1\n",
      "16115    0\n",
      "10802    0\n",
      "7160     0\n",
      "2499     0\n",
      "12899    0\n",
      "18149    1\n",
      "Name: sentiment, Length: 8250, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_lines = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = train['review'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    tokens = word_tokenize(line)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    review_lines.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngmat\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit=500000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_vector('sad').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n",
      "[[ 6  2  0  0]\n",
      " [ 3  1  0  0]\n",
      " [ 7  4  0  0]\n",
      " [ 8  1  0  0]\n",
      " [ 9  0  0  0]\n",
      " [10  0  0  0]\n",
      " [ 5  4  0  0]\n",
      " [11  3  0  0]\n",
      " [ 5  1  0  0]\n",
      " [12 13  2 14]]\n"
     ]
    }
   ],
   "source": [
    "# define documents\n",
    "docs = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!',\n",
    "        'Weak',\n",
    "        'Poor effort!',\n",
    "        'not good',\n",
    "        'poor work',\n",
    "        'Could have done better.']\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4 # which review is the longest number of words\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = model.get_vector(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.07568359  0.03369141 -0.06494141 ...  0.01202393  0.13574219\n",
      "  -0.09130859]\n",
      " [-0.04931641  0.24121094 -0.03222656 ... -0.04077148  0.00130463\n",
      "  -0.17871094]\n",
      " ...\n",
      " [ 0.12353516  0.03198242  0.15039062 ... -0.21289062 -0.00643921\n",
      "  -0.08007812]\n",
      " [-0.13964844 -0.03466797 -0.05371094 ... -0.11816406 -0.00245667\n",
      "  -0.07226562]\n",
      " [-0.19433594 -0.06152344  0.00915527 ... -0.08007812 -0.08740234\n",
      "  -0.00427246]]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('saddening', 0.7273085713386536),\n",
       " ('Sad', 0.6610826253890991),\n",
       " ('saddened', 0.6604381799697876),\n",
       " ('heartbreaking', 0.6573508381843567),\n",
       " ('disheartening', 0.6507317423820496),\n",
       " ('saddens_me', 0.6407119035720825),\n",
       " ('distressing', 0.6399092674255371),\n",
       " ('saddest', 0.6345508694648743),\n",
       " ('unfortunate', 0.6272087097167969),\n",
       " ('sorry', 0.619404673576355)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['sad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300  ##negative or positive a word is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length, weights=[embedding_matrix]))\n",
    "model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngmat\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 0.6881\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 601us/step - loss: 0.6724\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 499us/step - loss: 0.6807\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 399us/step - loss: 0.6628\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6510\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 499us/step - loss: 0.6542\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 497us/step - loss: 0.6261\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 499us/step - loss: 0.6235\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 399us/step - loss: 0.6149\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 896us/step - loss: 0.6045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1cfaebfda90>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_docs, labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
